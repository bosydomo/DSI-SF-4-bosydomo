{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float:  left; margin: 10px;\">\n",
    "\n",
    "# Test / Train / Split\n",
    "---\n",
    "Week 3 | Lesson 3.2\n",
    "\n",
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- **Describe** test/train/split and cross-validation\n",
    "- **Explain** why we want to use these validation techniques and how they differ\n",
    "- **Split** data into testing and training sets using both test/train/split and cross validation and **Apply** both techniques to score a model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Draw on board data points that will lead to severely overfit model. B0 +B1X1 + B1X1^2 + .... B1X1^15\n",
    "\n",
    "- Ask student to come up and draw line of best fit through every point\n",
    "- Ask student what R^2 of the model on this data is\n",
    "- Put a few more data points on board and ask students if they think is still a good model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LESSON GUIDE\n",
    "| TIMING  | TYPE  | TOPIC  |\n",
    "|:-:|---|---|\n",
    "| 5 min  | [Opening](#opening)  | Topic description  |\n",
    "| 10 min  | [Introduction](#introduction)   | Topic description  |\n",
    "| 15 min  | [Demo](#demo)  | Topic description  |\n",
    "| 25 min  | [Guided Practice](#guided-practice<a name=\"opening\"></a>)  | Topic description  |\n",
    "| 25 min  | [Independent Practice](#ind-practice)  | Topic description  |\n",
    "| 5 min  | [Conclusion](#conclusion)  | Topic description  |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://tomrobertshaw.net/img/2015/12/overfitting.jpg\" width=\"1000\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Ask class what is wrong with first model.\n",
    "- Severeley underfit model is very high bias. Our model falls short of true model of data\n",
    "\n",
    "Ask class what is wrong with third model.\n",
    "- Severely overfit model is very high variance. If we were to feed new data into this model it could look drastically different\n",
    "\n",
    "Middle model is a good compromise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://image.slidesharecdn.com/nncollovcapaldo2013-131220052427-phpapp01/95/machine-learning-introduction-to-neural-networks-12-638.jpg?cb=1393073301\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Introduction: Topic (10 mins)\n",
    "\n",
    "So far we've focused on fitting the **best model** to our data. In practice, we need\n",
    "to also make and **validate predictions** with our models. By **splitting our data** set\n",
    "into a **subset to train our model** on and a **subset to make and test predictions**\n",
    "on, we can **validate the effectiveness** of our model. This is called a **_train/test\n",
    "split_** and we'll explore a number of ways to effectively carry out the split.\n",
    "It's also a good way to avoid overfitting on your dataset (but not always).\n",
    "\n",
    "**Test/train split benefits:**\n",
    "* By saving a subset of data to make predictions you can verify predictions without having to collect new data **(which may be difficult or expensive)**\n",
    "* Can help avoid overfitting\n",
    "* Improve the quality of our predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Draw diagram on board showing data being segmented into test and training data. Emphasize that the model NEVER sees the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About Cross-Validation\n",
    "\n",
    "We use cross-validation to sample our data in order to understand how it may perform in a variety of cases, given a set of parameters.  It also helps us understand how predictions react to data.  Largely, we are \"testing\" how our model stands up to basic assumptions, given a model.\n",
    "\n",
    "Why Validate? \n",
    "\n",
    "- Test the model\n",
    "- Avoid overfitting\n",
    "- See how well a model generalizes to an independet dataset\n",
    "\n",
    "The goal of cross validation is to define a dataset to \"test\" the model in the training phase (i.e., the validation dataset), in order to limit problems like overfitting, give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Fold Validation\n",
    "\n",
    "Essentially, K-Fold splits our dataset up into K-Folds, and uses one segment to test.\n",
    "\n",
    "- Split data into K folds (subsets)\n",
    "- Use K-1 for training\n",
    "- Use 1 for testing\n",
    "- Repeat K times using the next segment for testing each time\n",
    "- Mean results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Draw 3 folds on board and have student come up and fill in training and test folds. Emphasize that all of the data is included in training 2 of the 3 models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Fold Validation In Action\n",
    "\n",
    "<img src=\"https://snag.gy/o1lLcw.jpg?convert_to_webp=true\" width=\"500\"a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick note about \"Hold Out\" validation\n",
    "\n",
    "Briefly, sometimes you'll hear about \"hold out\" validation.  Essentially, you ommit a section of data from any validation setup, in order to truly test unknown data against your model.\n",
    "\n",
    "The main reason you might try this:\n",
    "\n",
    "- Test your \"winning\" model against truly clean data\n",
    "- \"Truly clean\" is data a model has never seen \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTO NOTEBOOK!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (2 mins) Question:  Is 2-fold cross-validation the same as a 50:50 test/train split?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It may seem so at first glance, but with 2-fold cross-validation we get a\n",
    "prediction for every point since we use each half of the data to train and test\n",
    "separate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (2 mins) Question:  Will two different 50:50 (or x:y) splits produce the same model score?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consider these cases:\n",
    "- Splits on time of day\n",
    "- Splits on categorical variables\n",
    "- Splits on dataframes with different bit masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In general no, and if the splits are chosen poorly along a categorical variable, the difference could be very large. For example, theme park attendence might be very different depending on the day of the week. Can students think\n",
    "of other examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ADDITIONAL RESOURCES (SCIKIT)\n",
    "\n",
    "- [Cross-validation Example](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py)\n",
    "- [Plotting Cross-Validated Predictions](http://scikit-learn.org/stable/auto_examples/plot_cv_predict.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "#### Why do we want to use cross validation to test our models?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To try and avoid overfitting our model\n",
    "- To make sure our model generalizes well when we use it to predict on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### What are the two methods for cross validating our models? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Test/Train/Split\n",
    "    - Data is split into training and test sets (70/30, 60/40 perhaps) and model is trained on training data and scored on held out test data\n",
    "    - Test data is held out and **NEVER** used in constructing model    \n",
    "- K-Fold Cross Validation\n",
    "    - Data is split into K folds. Each fold is broken into K segments where K-1 segments are used for training the model with the last segment being used to test the data.\n",
    "    - Every segment is used as test data once and as training data K-1 times\n",
    "    - The model will eventually see every data point"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
